import streamlit as st
import torch
import torch.nn.functional as F
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import matplotlib.pyplot as plt

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if "input_ids" not in st.session_state:
    st.session_state.input_ids = None
if "generated_tokens" not in st.session_state:
    st.session_state.generated_tokens = []
if "steps" not in st.session_state:
    # å„ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãƒªã‚¹ãƒˆ
    # è¦ç´ ã¯ dict: {"topk_tokens","topk_values","chosen_id","attn_avg","tokens_all"}
    st.session_state.steps = []
if "step_index" not in st.session_state:
    st.session_state.step_index = 0

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ¼åˆæœŸåŒ–
if "prompt" not in st.session_state:
    st.session_state.prompt = "The cat sat on the"
if "prompt_initialized" not in st.session_state:
    st.session_state.prompt_initialized = False

@st.cache_resource
def load_model():
    model = GPT2LMHeadModel.from_pretrained("gpt2-medium", output_attentions=True)
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
    model.eval()
    return model, tokenizer

model, tokenizer = load_model()

st.title("ğŸ” GPT-2 å¯è¦–åŒ–ãƒ‡ãƒ¢ï¼šã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ + Temperature=0å¯¾å¿œ")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠï¼†å…¥åŠ›æ¬„
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
example_prompt = st.selectbox(
    "ğŸ§ª è©¦ã—ã¦ã¿ãŸã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸ã‚“ã§ãã ã•ã„ï¼ˆç·¨é›†ã‚‚å¯èƒ½ï¼‰",
    [
        "ï¼ˆâ†é¸ã‚“ã§ãã ã•ã„ï¼‰",
        "Once upon a time, there was a",
        "In the future, artificial intelligence will",
        "The quick brown fox jumps over the",
        "I can't believe that she actually",
        "This is the reason why you should never",
        "The meaning of life is",
        "If I were the president, I would",
        "She looked at him and said",
    ],
    key="prompt_selector"
)
if example_prompt != "ï¼ˆâ†é¸ã‚“ã§ãã ã•ã„ï¼‰" and not st.session_state.prompt_initialized:
    st.session_state.prompt = example_prompt
    st.session_state.prompt_initialized = True

prompt = st.text_input(
    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
    value=st.session_state.prompt
)
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›´æ¥ç·¨é›†ã—ãŸã‚‰ä¸Šæ›¸ã
st.session_state.prompt = prompt

temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)

top_p = st.slider(
    "Top-p (Nucleus sampling)", 0.0, 1.0, 1.0, 0.01,
    help="Top-p < 1.0 ã®ã¨ãã¯ Top-p ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€Top-p = 1.0 ã®ã¨ãã¯ Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"
)
top_k = st.slider(
    "Top-K sampling", 1, 50, 10, 1,
    help="Top-p = 1.0 ã®ã¨ãã®ã¿æœ‰åŠ¹", disabled=(top_p < 1.0)
)
if top_p < 1.0:
    st.markdown("âš ï¸ Top-K ã¯ç¾åœ¨ç„¡åŠ¹ã§ã™ï¼ˆTop-p æœ‰åŠ¹ï¼‰")
else:
    st.markdown("âš ï¸ Top-p ã¯ç¾åœ¨ç„¡åŠ¹ã§ã™ï¼ˆTop-K æœ‰åŠ¹ï¼‰")

st.markdown("---")

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæœŸåŒ–ãƒœã‚¿ãƒ³
if st.button("ğŸ”„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæœŸåŒ–"):
    st.session_state.input_ids = tokenizer.encode(st.session_state.prompt, return_tensors="pt")
    st.session_state.generated_tokens = []
    st.session_state.steps = []
    st.session_state.step_index = 0

if st.session_state.input_ids is None:
    st.warning("ã¾ãšã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# å¯è¦–åŒ–ç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
chart_placeholder = st.empty()
attention_placeholder = st.empty()

# ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆ
if st.button("â–¶ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ"):
    input_ids = st.session_state.input_ids
    with torch.no_grad():
        outputs = model(input_ids, output_attentions=True)
        raw_logits = outputs.logits[:, -1, :]

        # Temperature=0 â†’ å®Œå…¨ã« Greedy
        if temperature < 1e-5:
            # raw_logits ã‚’ãã®ã¾ã¾ä½¿ã£ã¦ argmax
            next_token = torch.argmax(raw_logits, dim=-1, keepdim=True)
            probs = None
        else:
            # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°â†’Softmax
            scaled_logits = raw_logits / temperature
            probs = F.softmax(scaled_logits, dim=-1)

            # Top-p ã¾ãŸã¯ Top-K ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_p < 1.0:
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                cutoff = cumulative_probs > top_p
                cutoff_idx = torch.argmax(cutoff.int()).item() + 1
                nucleus_indices = sorted_indices[0, :cutoff_idx]
                filtered_probs = torch.zeros_like(probs)
                filtered_probs[0, nucleus_indices] = probs[0, nucleus_indices]
            else:
                top_probs, top_indices = torch.topk(probs, top_k)
                filtered_probs = torch.zeros_like(probs)
                filtered_probs[0, top_indices[0]] = probs[0, top_indices[0]]

            filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)
            next_token = torch.multinomial(filtered_probs, num_samples=1)

        st.session_state.input_ids = torch.cat([input_ids, next_token], dim=1)
        st.session_state.generated_tokens.append(next_token.item())

        # Top-Kï¼ˆå¯è¦–åŒ–ç”¨ã¨ã—ã¦å¸¸ã« Top-K ã‚’å–å¾—ï¼‰
        if probs is not None:
            topk_probs, topk_indices = torch.topk(probs, top_k)
        else:
            # Temperature=0 ã§ã¯ raw_logits ã‹ã‚‰ Top-K
            topk_probs, topk_indices = torch.topk(raw_logits, top_k)
        topk_tokens = [tokenizer.decode([i]).strip() for i in topk_indices[0]]
        topk_values = (topk_probs[0].tolist() if probs is not None else topk_probs[0].tolist())
        chosen_id = next_token.item()

        # Attention è¡Œåˆ—ï¼ˆæœ€çµ‚å±¤ã®å¹³å‡ï¼‰
        attn = outputs.attentions[-1][0]  # shape: [n_head, seq_len, seq_len]
        attn_avg = attn.mean(dim=0).cpu().numpy()
        tokens_all = [tokenizer.decode([i]).strip() for i in st.session_state.input_ids[0].tolist()]

        # ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        step_data = {
            "topk_tokens": topk_tokens,
            "topk_values": topk_values,
            "topk_ids": topk_indices[0].tolist(),
            "chosen_id": chosen_id,
            "attn_avg": attn_avg,
            "tokens_all": tokens_all,
        }
        st.session_state.steps.append(step_data)
        st.session_state.step_index = len(st.session_state.steps) - 1

# ã‚¹ãƒ†ãƒƒãƒ—ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
if st.session_state.steps:
    idx = st.session_state.step_index
    step = st.session_state.steps[idx]
    col1, col2, col3 = st.columns([1, 2, 1])
    with col1:
        prev_disabled = idx == 0
        if st.button("â† å‰ã¸", disabled=prev_disabled) and idx > 0:
            st.session_state.step_index = idx - 1
    with col3:
        next_disabled = idx == len(st.session_state.steps) - 1
        if st.button("æ¬¡ã¸ â†’", disabled=next_disabled) and idx < len(st.session_state.steps) - 1:
            st.session_state.step_index = idx + 1
    st.markdown(f"**Step {idx+1}/{len(st.session_state.steps)}**")

    # ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆ
    if top_p < 1.0:
        title = f"Step {idx+1}: Next Token Candidates (Top-p)"
    else:
        title = f"Step {idx+1}: Next Token Candidates (Top-K)"

    # åˆ†å¸ƒæ£’ã‚°ãƒ©ãƒ•ã®å†æç”»
    fig, ax = plt.subplots()
    colors = [
        "red" if tok_id == step["chosen_id"] else "gray"
        for tok_id in step["topk_ids"]
    ]
    ax.barh(step["topk_tokens"][::-1], step["topk_values"][::-1], color=colors[::-1])
    ax.set_title(title)
    ax.set_xlabel("Probability or Logit Score")
    ax.invert_yaxis()
    chart_placeholder.pyplot(fig)

    # Attention ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å†æç”»
    fig2, ax2 = plt.subplots(figsize=(6, 5))
    im = ax2.imshow(step["attn_avg"], cmap="viridis", vmin=0.0, vmax=0.2)
    ax2.set_xticks(range(len(step["tokens_all"])))
    ax2.set_xticklabels(step["tokens_all"], rotation=90, fontsize=6)
    ax2.set_yticks(range(len(step["tokens_all"])))
    ax2.set_yticklabels(step["tokens_all"], fontsize=6)
    ax2.set_title(f"Step {idx+1}: Attention Map")
    fig2.colorbar(im, ax=ax2)
    attention_placeholder.pyplot(fig2)

# æœ€çµ‚å‡ºåŠ›æ–‡
st.markdown("### ğŸ§  æœ€çµ‚çš„ãªå‡ºåŠ›æ–‡")
st.write(tokenizer.decode(st.session_state.input_ids[0], skip_special_tokens=True))
