import streamlit as st
import torch
import torch.nn.functional as F
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import matplotlib
import matplotlib.pyplot as plt

# matplotlib.rcParams["font.family"] = "IPAexGothic"

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if "input_ids" not in st.session_state:
    st.session_state.input_ids = None
if "generated_tokens" not in st.session_state:
    st.session_state.generated_tokens = []
if "mode" not in st.session_state:
    st.session_state.mode = "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—"
if "steps" not in st.session_state:
    # å„ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãƒªã‚¹ãƒˆ
    # è¦ç´ ã¯ dict: {"topk_tokens","topk_values","chosen_id","attn_avg","tokens_all"}
    st.session_state.steps = []
if "step_index" not in st.session_state:
    st.session_state.step_index = 0

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ¼åˆæœŸåŒ–
if "prompt" not in st.session_state:
    st.session_state.prompt = "The cat sat on the"
if "prompt_initialized" not in st.session_state:
    st.session_state.prompt_initialized = False

@st.cache_resource
def load_model():
    model = GPT2LMHeadModel.from_pretrained("gpt2-medium", output_attentions=True)
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
    model.eval()
    return model, tokenizer

model, tokenizer = load_model()

st.title("ğŸ” GPT-2 å¯è¦–åŒ–ãƒ‡ãƒ¢ï¼š2ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ï¼‹ã‚¹ãƒ†ãƒƒãƒ—ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠï¼†å…¥åŠ›æ¬„
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
example_prompt = st.selectbox(
    "ğŸ§ª è©¦ã—ã¦ã¿ãŸã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸ã‚“ã§ãã ã•ã„ï¼ˆç·¨é›†ã‚‚å¯èƒ½ï¼‰",
    [
        "ï¼ˆâ†é¸ã‚“ã§ãã ã•ã„ï¼‰",
        "Once upon a time, there was a",
        "In the future, artificial intelligence will",
        "The quick brown fox jumps over the",
        "I can't believe that she actually",
        "This is the reason why you should never",
        "The meaning of life is",
        "If I were the president, I would",
        "She looked at him and said",
    ],
    key="prompt_selector"
)
if example_prompt != "ï¼ˆâ†é¸ã‚“ã§ãã ã•ã„ï¼‰" and not st.session_state.prompt_initialized:
    st.session_state.prompt = example_prompt
    st.session_state.prompt_initialized = True

prompt = st.text_input(
    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
    value=st.session_state.prompt
)
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›´æ¥ç·¨é›†ã—ãŸã‚‰ä¸Šæ›¸ã
st.session_state.prompt = prompt

temperature = st.slider("Temperature", 0.1, 2.0, 0.7, 0.1)

# ãƒ¢ãƒ¼ãƒ‰ã”ã¨ã®è¨­å®š
mode = st.radio(
    "ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
    ("ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—", "ã¾ã¨ã‚ã¦ç”Ÿæˆ")
)
st.session_state.mode = mode

if mode == "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—":
    top_p = st.slider(
        "Top-p (Nucleus sampling)", 0.0, 1.0, 1.0, 0.01,
        help="Top-p < 1.0 ã®ã¨ãã¯ Top-p ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€Top-p = 1.0 ã®ã¨ãã¯ Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"
    )
    top_k = st.slider(
        "Top-K sampling", 1, 50, 10, 1,
        help="Top-p = 1.0 ã®ã¨ãã®ã¿æœ‰åŠ¹", disabled=(top_p < 1.0)
    )
    if top_p < 1.0:
        st.markdown("âš ï¸ Top-K ã¯ç¾åœ¨ç„¡åŠ¹ã§ã™ï¼ˆTop-p æœ‰åŠ¹ï¼‰")
    else:
        st.markdown("âš ï¸ Top-p ã¯ç¾åœ¨ç„¡åŠ¹ã§ã™ï¼ˆTop-K æœ‰åŠ¹ï¼‰")
else:
    gen_count = st.slider(
        "ã¾ã¨ã‚ã¦ç”Ÿæˆã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°", 1, 50, 20, 1,
        help="ç”Ÿæˆã—ãŸã„ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®š"
    )
    top_p = st.slider(
        "Top-p (Nucleus sampling)", 0.0, 1.0, 0.9, 0.01,
        help="ã¾ã¨ã‚ã¦ç”Ÿæˆæ™‚ã® Top-p è¨­å®š"
    )
    top_k = st.slider(
        "Top-K sampling", 1, 50, 40, 1,
        help="ã¾ã¨ã‚ã¦ç”Ÿæˆæ™‚ã® Top-K è¨­å®šï¼ˆTop-p = 1.0 ã®ã¨ãæœ‰åŠ¹ï¼‰", disabled=(top_p < 1.0)
    )
    if top_p < 1.0:
        st.markdown("âš ï¸ ã¾ã¨ã‚ã¦ç”Ÿæˆã§ã¯ Top-K ã¯ç„¡åŠ¹ã§ã™ï¼ˆTop-p æœ‰åŠ¹ï¼‰")
    else:
        st.markdown("âš ï¸ ã¾ã¨ã‚ã¦ç”Ÿæˆã§ã¯ Top-p ã¯ç„¡åŠ¹ã§ã™ï¼ˆTop-K æœ‰åŠ¹ï¼‰")

st.markdown("---")

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæœŸåŒ–ãƒœã‚¿ãƒ³
if st.button("ğŸ”„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæœŸåŒ–"):
    st.session_state.input_ids = tokenizer.encode(st.session_state.prompt, return_tensors="pt")
    st.session_state.generated_tokens = []
    st.session_state.steps = []
    st.session_state.step_index = 0

if st.session_state.input_ids is None:
    st.warning("ã¾ãšã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# å¯è¦–åŒ–ç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
chart_placeholder = st.empty()
attention_placeholder = st.empty()

if mode == "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—":
    # ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ç”Ÿæˆãƒœã‚¿ãƒ³
    if st.button("â–¶ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ"):
        input_ids = st.session_state.input_ids
        with torch.no_grad():
            outputs = model(input_ids, output_attentions=True)
            logits = outputs.logits[:, -1, :] / temperature
            probs = F.softmax(logits, dim=-1)

            # Top-p ã¾ãŸã¯ Top-K ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_p < 1.0:
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                cutoff = cumulative_probs > top_p
                cutoff_idx = torch.argmax(cutoff.int()).item() + 1
                nucleus_indices = sorted_indices[0, :cutoff_idx]
                filtered_probs = torch.zeros_like(probs)
                filtered_probs[0, nucleus_indices] = probs[0, nucleus_indices]
            else:
                top_probs, top_indices = torch.topk(probs, top_k)
                filtered_probs = torch.zeros_like(probs)
                filtered_probs[0, top_indices[0]] = probs[0, top_indices[0]]

            filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)
            next_token = torch.multinomial(filtered_probs, num_samples=1)
            st.session_state.input_ids = torch.cat([input_ids, next_token], dim=1)
            st.session_state.generated_tokens.append(next_token.item())

            # Top-Kï¼ˆå¯è¦–åŒ–ç”¨ã¨ã—ã¦å¸¸ã« Top-K ã‚’å–å¾—ï¼‰
            topk_probs, topk_indices = torch.topk(probs, top_k)
            topk_tokens = [tokenizer.decode([i]).strip() for i in topk_indices[0]]
            topk_values = topk_probs[0].tolist()
            chosen_id = next_token.item()

            # Attention è¡Œåˆ—ï¼ˆæœ€çµ‚å±¤ã®å¹³å‡ï¼‰
            attn = outputs.attentions[-1][0]  # shape: [n_head, seq_len, seq_len]
            attn_avg = attn.mean(dim=0).cpu().numpy()
            tokens_all = [tokenizer.decode([i]).strip() for i in st.session_state.input_ids[0].tolist()]

            # ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            step_data = {
                "topk_tokens": topk_tokens,
                "topk_values": topk_values,
                "topk_ids": topk_indices[0].tolist(),
                "chosen_id": chosen_id,
                "attn_avg": attn_avg,
                "tokens_all": tokens_all,
            }
            st.session_state.steps.append(step_data)
            st.session_state.step_index = len(st.session_state.steps) - 1

    # ã‚¹ãƒ†ãƒƒãƒ—ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
    if st.session_state.steps:
        idx = st.session_state.step_index
        step = st.session_state.steps[idx]
        col1, col2, col3 = st.columns([1, 2, 1])
        with col1:
            prev_disabled = idx == 0
            if st.button("â† å‰ã¸", disabled=prev_disabled) and idx > 0:
                st.session_state.step_index = idx - 1
        with col3:
            next_disabled = idx == len(st.session_state.steps) - 1
            if st.button("æ¬¡ã¸ â†’", disabled=next_disabled) and idx < len(st.session_state.steps) - 1:
                st.session_state.step_index = idx + 1
        st.markdown(f"**Step {idx+1}/{len(st.session_state.steps)}**")

        # ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆ
        if top_p < 1.0:
            title = f"Step {idx+1}: Next Token Candidates (Top-p)"
        else:
            title = f"Step {idx+1}: Next Token Candidates (Top-K)"

        # åˆ†å¸ƒæ£’ã‚°ãƒ©ãƒ•ã®å†æç”»
        fig, ax = plt.subplots()
        colors = [
            "red" if tok_id == step["chosen_id"] else "gray"
            for tok_id in step["topk_ids"]
        ]
        ax.barh(step["topk_tokens"][::-1], step["topk_values"][::-1], color=colors[::-1])
        ax.set_title(title)
        ax.set_xlabel("Probability")
        ax.invert_yaxis()
        chart_placeholder.pyplot(fig)

        # Attention ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å†æç”»
        fig2, ax2 = plt.subplots(figsize=(6, 5))
        im = ax2.imshow(step["attn_avg"], cmap="viridis", vmin=0.0, vmax=0.2)
        ax2.set_xticks(range(len(step["tokens_all"])))
        ax2.set_xticklabels(step["tokens_all"], rotation=90, fontsize=6)
        ax2.set_yticks(range(len(step["tokens_all"])))
        ax2.set_yticklabels(step["tokens_all"], fontsize=6)
        ax2.set_title(f"Step {idx+1}: Attention Map")
        fig2.colorbar(im, ax=ax2)
        attention_placeholder.pyplot(fig2)

else:
    if st.button("â–¶ï¸ ã¾ã¨ã‚ã¦ç”Ÿæˆ"):
        input_ids = st.session_state.input_ids
        with torch.no_grad():
            if top_p < 1.0:
                # Top-p ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                generated_ids = model.generate(
                    input_ids,
                    max_new_tokens=gen_count,
                    do_sample=True,
                    top_p=top_p,
                    temperature=temperature,
                    pad_token_id=tokenizer.eos_token_id
                )
            else:
                # Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                generated_ids = model.generate(
                    input_ids,
                    max_new_tokens=gen_count,
                    do_sample=True,
                    top_k=top_k,
                    temperature=temperature,
                    pad_token_id=tokenizer.eos_token_id
                )

        # ç”Ÿæˆéƒ¨åˆ†ã‚’ãƒªã‚¹ãƒˆã§æŠ½å‡º
        new_ids = generated_ids[0][input_ids.size(1):].tolist()
        seq = input_ids
        st.session_state.steps = []

        # å„ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        for idx, tok_id in enumerate(new_ids):
            with torch.no_grad():
                outputs = model(seq, output_attentions=True)
                logits = outputs.logits[:, -1, :] / temperature
                probs = F.softmax(logits, dim=-1)

                topk_probs, topk_indices = torch.topk(probs, top_k)
                topk_tokens = [tokenizer.decode([i]).strip() for i in topk_indices[0]]
                topk_values = topk_probs[0].tolist()
                topk_ids = topk_indices[0].tolist()
                chosen_id = tok_id

                attn = outputs.attentions[-1][0]
                attn_avg = attn.mean(dim=0).cpu().numpy()
                tokens_all = [tokenizer.decode([i]).strip() for i in seq[0].tolist()]

                step_data = {
                    "topk_tokens": topk_tokens,
                    "topk_values": topk_values,
                    "topk_ids": topk_ids,
                    "chosen_id": chosen_id,
                    "attn_avg": attn_avg,
                    "tokens_all": tokens_all,
                }
                st.session_state.steps.append(step_data)
                seq = torch.cat([seq, torch.tensor([[tok_id]])], dim=1)

        st.session_state.input_ids = seq.clone()
        st.session_state.step_index = 0

    # ã¾ã¨ã‚ã¦ç”Ÿæˆå¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
    if st.session_state.steps:
        idx = st.session_state.step_index
        step = st.session_state.steps[idx]
        col1, col2, col3 = st.columns([1, 2, 1])
        with col1:
            prev_disabled = idx == 0
            if st.button("â† å‰ã¸", disabled=prev_disabled) and idx > 0:
                st.session_state.step_index = idx - 1
        with col3:
            next_disabled = idx == len(st.session_state.steps) - 1
            if st.button("æ¬¡ã¸ â†’", disabled=next_disabled) and idx < len(st.session_state.steps) - 1:
                st.session_state.step_index = idx + 1
        st.markdown(f"**Step {idx+1}/{len(st.session_state.steps)}**")

        # ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆ
        if top_p < 1.0:
            title = f"Step {idx+1}: Next Token Candidates (Top-p)"
        else:
            title = f"Step {idx+1}: Next Token Candidates (Top-K)"

        # åˆ†å¸ƒæ£’ã‚°ãƒ©ãƒ•ã®å†æç”»
        fig, ax = plt.subplots()
        colors = [
            "red" if tok_id == step["chosen_id"] else "gray"
            for tok_id in step["topk_ids"]
        ]
        ax.barh(step["topk_tokens"][::-1], step["topk_values"][::-1], color=colors[::-1])
        ax.set_title(title)
        ax.set_xlabel("Probability")
        ax.invert_yaxis()
        chart_placeholder.pyplot(fig)

        # Attention ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å†æç”»
        fig2, ax2 = plt.subplots(figsize=(6, 5))
        im = ax2.imshow(step["attn_avg"], cmap="viridis", vmin=0.0, vmax=0.2)
        ax2.set_xticks(range(len(step["tokens_all"])))
        ax2.set_xticklabels(step["tokens_all"], rotation=90, fontsize=6)
        ax2.set_yticks(range(len(step["tokens_all"])))
        ax2.set_yticklabels(step["tokens_all"], fontsize=6)
        ax2.set_title(f"Step {idx+1}: Attention Map")
        fig2.colorbar(im, ax=ax2)
        attention_placeholder.pyplot(fig2)

# æœ€çµ‚å‡ºåŠ›æ–‡
st.markdown("### ğŸ§  æœ€çµ‚çš„ãªå‡ºåŠ›æ–‡")
st.write(tokenizer.decode(st.session_state.input_ids[0], skip_special_tokens=True))
